{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 模型调用的分类\n",
    "1. 模型功能的不同：\n",
    "\n",
    "非对话模型（LLMs、Text Model）\n",
    "\n",
    "对话模型（Chat Model）（推荐）\n",
    "\n",
    "嵌入模型（Embedding Model）（RAG chapter）\n",
    "\n",
    "2. 模型调用时，参数书写的位置不同（API Key、base URL、Model-Name）\n",
    "\n",
    "硬编码（参数写在代码中，不用）\n",
    "\n",
    "环境变量\n",
    "\n",
    "配置文件（.env）（推荐）\n",
    "\n",
    "3. 具体API的调用\n",
    "\n",
    "使用Langchain 提供的API（推荐方式）\n",
    "\n",
    "使用OpenAI 官方API 的方式\n",
    "\n",
    "使用其他平台提供的 API（阿里云百炼等）"
   ],
   "id": "7440335aab1783f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3c710bad768f50a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 非对话模型",
   "id": "4d97e3cf20d098a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T16:59:23.747041Z",
     "start_time": "2025-09-28T16:59:23.327238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "llm = OpenAI()\n",
    "res = llm.invoke(\"写一首关于秋天的诗\")\n",
    "print(res.content)\n",
    "print(type(res))"
   ],
   "id": "1c45dd0b2ee8a00b",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'model_dump'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      7\u001B[39m os.environ[\u001B[33m\"\u001B[39m\u001B[33mOPENAI_BASE_URL\u001B[39m\u001B[33m\"\u001B[39m] = os.getenv(\u001B[33m\"\u001B[39m\u001B[33mOPENAI_BASE_URL\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      9\u001B[39m llm = OpenAI()\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m res = \u001B[43mllm\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m写一首关于秋天的诗\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[38;5;28mprint\u001B[39m(res.content)\n\u001B[32m     12\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mtype\u001B[39m(res))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/langchain_core/language_models/llms.py:392\u001B[39m, in \u001B[36mBaseLLM.invoke\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    381\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    382\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minvoke\u001B[39m(\n\u001B[32m    383\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    388\u001B[39m     **kwargs: Any,\n\u001B[32m    389\u001B[39m ) -> \u001B[38;5;28mstr\u001B[39m:\n\u001B[32m    390\u001B[39m     config = ensure_config(config)\n\u001B[32m    391\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[32m--> \u001B[39m\u001B[32m392\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    393\u001B[39m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    394\u001B[39m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    395\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcallbacks\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    396\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtags\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    397\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    398\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_name\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    399\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_id\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    400\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    401\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    402\u001B[39m         .generations[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m]\n\u001B[32m    403\u001B[39m         .text\n\u001B[32m    404\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/langchain_core/language_models/llms.py:791\u001B[39m, in \u001B[36mBaseLLM.generate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m    782\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    783\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_prompt\u001B[39m(\n\u001B[32m    784\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    788\u001B[39m     **kwargs: Any,\n\u001B[32m    789\u001B[39m ) -> LLMResult:\n\u001B[32m    790\u001B[39m     prompt_strings = [p.to_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m--> \u001B[39m\u001B[32m791\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_strings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/langchain_core/language_models/llms.py:1002\u001B[39m, in \u001B[36mBaseLLM.generate\u001B[39m\u001B[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m    987\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m.cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m get_llm_cache() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m.cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[32m    988\u001B[39m     run_managers = [\n\u001B[32m    989\u001B[39m         callback_manager.on_llm_start(\n\u001B[32m    990\u001B[39m             \u001B[38;5;28mself\u001B[39m._serialized,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1000\u001B[39m         )\n\u001B[32m   1001\u001B[39m     ]\n\u001B[32m-> \u001B[39m\u001B[32m1002\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1003\u001B[39m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1004\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1005\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1006\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1007\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1008\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1009\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) > \u001B[32m0\u001B[39m:\n\u001B[32m   1010\u001B[39m     run_managers = [\n\u001B[32m   1011\u001B[39m         callback_managers[idx].on_llm_start(\n\u001B[32m   1012\u001B[39m             \u001B[38;5;28mself\u001B[39m._serialized,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1019\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m missing_prompt_idxs\n\u001B[32m   1020\u001B[39m     ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/langchain_core/language_models/llms.py:817\u001B[39m, in \u001B[36mBaseLLM._generate_helper\u001B[39m\u001B[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[39m\n\u001B[32m    806\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_generate_helper\u001B[39m(\n\u001B[32m    807\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    808\u001B[39m     prompts: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m],\n\u001B[32m   (...)\u001B[39m\u001B[32m    813\u001B[39m     **kwargs: Any,\n\u001B[32m    814\u001B[39m ) -> LLMResult:\n\u001B[32m    815\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    816\u001B[39m         output = (\n\u001B[32m--> \u001B[39m\u001B[32m817\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    818\u001B[39m \u001B[43m                \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    819\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    820\u001B[39m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# TODO: support multiple run managers\u001B[39;49;00m\n\u001B[32m    821\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    822\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    823\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    824\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m    825\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._generate(prompts, stop=stop)\n\u001B[32m    826\u001B[39m         )\n\u001B[32m    827\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    828\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/langchain_openai/llms/base.py:450\u001B[39m, in \u001B[36mBaseOpenAI._generate\u001B[39m\u001B[34m(self, prompts, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m    446\u001B[39m response = \u001B[38;5;28mself\u001B[39m.client.create(prompt=_prompts, **params)\n\u001B[32m    447\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[32m    448\u001B[39m     \u001B[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001B[39;00m\n\u001B[32m    449\u001B[39m     \u001B[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m450\u001B[39m     response = \u001B[43mresponse\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmodel_dump\u001B[49m()\n\u001B[32m    452\u001B[39m \u001B[38;5;66;03m# Sometimes the AI Model calling will get error, we should raise it.\u001B[39;00m\n\u001B[32m    453\u001B[39m \u001B[38;5;66;03m# Otherwise, the next code 'choices.extend(response[\"choices\"])'\u001B[39;00m\n\u001B[32m    454\u001B[39m \u001B[38;5;66;03m# will throw a \"TypeError: 'NoneType' object is not iterable\" error\u001B[39;00m\n\u001B[32m    455\u001B[39m \u001B[38;5;66;03m# to mask the true error. Because 'response[\"choices\"]' is None.\u001B[39;00m\n\u001B[32m    456\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m response.get(\u001B[33m\"\u001B[39m\u001B[33merror\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[31mAttributeError\u001B[39m: 'str' object has no attribute 'model_dump'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 对话模型",
   "id": "867a85e7f4bd298e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T17:07:01.225054Z",
     "start_time": "2025-09-28T17:07:00.757264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "chat_model = ChatOpenAI(model_name='gpt-4o-mini')\n",
    "messages = [\n",
    "    SystemMessage(content=\"我是一个人工智能客服，我叫小智\"),\n",
    "    HumanMessage(content=\"你好，我是小红，很不高兴认识你\")\n",
    "]\n",
    "\n",
    "response = (chat_model.invoke(messages))\n",
    "print(response.content)\n",
    "print(type(response))"
   ],
   "id": "976c988ef6936ec9",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'model_dump'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m     10\u001B[39m chat_model = ChatOpenAI(model_name=\u001B[33m'\u001B[39m\u001B[33mgpt-4o-mini\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     11\u001B[39m messages = [\n\u001B[32m     12\u001B[39m     SystemMessage(content=\u001B[33m\"\u001B[39m\u001B[33m我是一个人工智能客服，我叫小智\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m     13\u001B[39m     HumanMessage(content=\u001B[33m\"\u001B[39m\u001B[33m你好，我是小红，很不高兴认识你\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     14\u001B[39m ]\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m response = (\u001B[43mchat_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m     17\u001B[39m \u001B[38;5;28mprint\u001B[39m(response.content)\n\u001B[32m     18\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mtype\u001B[39m(response))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:440\u001B[39m, in \u001B[36mBaseChatModel.invoke\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    426\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    427\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minvoke\u001B[39m(\n\u001B[32m    428\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    433\u001B[39m     **kwargs: Any,\n\u001B[32m    434\u001B[39m ) -> AIMessage:\n\u001B[32m    435\u001B[39m     config = ensure_config(config)\n\u001B[32m    436\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[32m    437\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAIMessage\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    438\u001B[39m         cast(\n\u001B[32m    439\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mChatGeneration\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m--> \u001B[39m\u001B[32m440\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    441\u001B[39m \u001B[43m                \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    442\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    443\u001B[39m \u001B[43m                \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcallbacks\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    444\u001B[39m \u001B[43m                \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtags\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    445\u001B[39m \u001B[43m                \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    446\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_name\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    447\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_id\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    448\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    449\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m.generations[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m],\n\u001B[32m    450\u001B[39m         ).message,\n\u001B[32m    451\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1122\u001B[39m, in \u001B[36mBaseChatModel.generate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m   1113\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m   1114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_prompt\u001B[39m(\n\u001B[32m   1115\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1119\u001B[39m     **kwargs: Any,\n\u001B[32m   1120\u001B[39m ) -> LLMResult:\n\u001B[32m   1121\u001B[39m     prompt_messages = [p.to_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m-> \u001B[39m\u001B[32m1122\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:939\u001B[39m, in \u001B[36mBaseChatModel.generate\u001B[39m\u001B[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m    936\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n\u001B[32m    937\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    938\u001B[39m         results.append(\n\u001B[32m--> \u001B[39m\u001B[32m939\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    940\u001B[39m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    941\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    942\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    943\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    944\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    945\u001B[39m         )\n\u001B[32m    946\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    947\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1216\u001B[39m, in \u001B[36mBaseChatModel._generate_with_cache\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1214\u001B[39m     result = generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[32m   1215\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m inspect.signature(\u001B[38;5;28mself\u001B[39m._generate).parameters.get(\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1216\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1217\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m   1218\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1219\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1220\u001B[39m     result = \u001B[38;5;28mself\u001B[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1214\u001B[39m, in \u001B[36mBaseChatOpenAI._generate\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1208\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   1209\u001B[39m     \u001B[38;5;28mself\u001B[39m.include_response_headers\n\u001B[32m   1210\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m raw_response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1211\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(raw_response, \u001B[33m\"\u001B[39m\u001B[33mheaders\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1212\u001B[39m ):\n\u001B[32m   1213\u001B[39m     generation_info = {\u001B[33m\"\u001B[39m\u001B[33mheaders\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mdict\u001B[39m(raw_response.headers)}\n\u001B[32m-> \u001B[39m\u001B[32m1214\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_create_chat_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgeneration_info\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1271\u001B[39m, in \u001B[36mBaseChatOpenAI._create_chat_result\u001B[39m\u001B[34m(self, response, generation_info)\u001B[39m\n\u001B[32m   1263\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_create_chat_result\u001B[39m(\n\u001B[32m   1264\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1265\u001B[39m     response: Union[\u001B[38;5;28mdict\u001B[39m, openai.BaseModel],\n\u001B[32m   1266\u001B[39m     generation_info: Optional[\u001B[38;5;28mdict\u001B[39m] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1267\u001B[39m ) -> ChatResult:\n\u001B[32m   1268\u001B[39m     generations = []\n\u001B[32m   1270\u001B[39m     response_dict = (\n\u001B[32m-> \u001B[39m\u001B[32m1271\u001B[39m         response \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mresponse\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmodel_dump\u001B[49m()\n\u001B[32m   1272\u001B[39m     )\n\u001B[32m   1273\u001B[39m     \u001B[38;5;66;03m# Sometimes the AI Model calling will get error, we should raise it (this is\u001B[39;00m\n\u001B[32m   1274\u001B[39m     \u001B[38;5;66;03m# typically followed by a null value for `choices`, which we raise for\u001B[39;00m\n\u001B[32m   1275\u001B[39m     \u001B[38;5;66;03m# separately below).\u001B[39;00m\n\u001B[32m   1276\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m response_dict.get(\u001B[33m\"\u001B[39m\u001B[33merror\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[31mAttributeError\u001B[39m: 'str' object has no attribute 'model_dump'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "158eff4cae458602"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embedding Model",
   "id": "1d9449090fe00dca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T17:10:09.352587Z",
     "start_time": "2025-09-28T17:10:05.938215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "res1 = embeddings_model.embed_query(\"我是文档中的数据\")\n",
    "print(res1)"
   ],
   "id": "54e9bcfc913840f5",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m      7\u001B[39m os.environ[\u001B[33m\"\u001B[39m\u001B[33mOPENAI_BASE_URL\u001B[39m\u001B[33m\"\u001B[39m] = os.getenv(\u001B[33m\"\u001B[39m\u001B[33mOPENAI_BASE_URL\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      9\u001B[39m embeddings_model = OpenAIEmbeddings(\n\u001B[32m     10\u001B[39m     model=\u001B[33m\"\u001B[39m\u001B[33mtext-embedding-ada-002\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     11\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m res1 = \u001B[43membeddings_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43membed_query\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m我是文档中的数据\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[38;5;28mprint\u001B[39m(res1)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:639\u001B[39m, in \u001B[36mOpenAIEmbeddings.embed_query\u001B[39m\u001B[34m(self, text, **kwargs)\u001B[39m\n\u001B[32m    629\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34membed_query\u001B[39m(\u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m, **kwargs: Any) -> \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfloat\u001B[39m]:\n\u001B[32m    630\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001B[39;00m\n\u001B[32m    631\u001B[39m \n\u001B[32m    632\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    637\u001B[39m \u001B[33;03m        Embedding for the text.\u001B[39;00m\n\u001B[32m    638\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m639\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43membed_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:591\u001B[39m, in \u001B[36mOpenAIEmbeddings.embed_documents\u001B[39m\u001B[34m(self, texts, chunk_size, **kwargs)\u001B[39m\n\u001B[32m    588\u001B[39m \u001B[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001B[39;00m\n\u001B[32m    589\u001B[39m \u001B[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001B[39;00m\n\u001B[32m    590\u001B[39m engine = cast(\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mself\u001B[39m.deployment)\n\u001B[32m--> \u001B[39m\u001B[32m591\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_len_safe_embeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    592\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[43m=\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    593\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:479\u001B[39m, in \u001B[36mOpenAIEmbeddings._get_len_safe_embeddings\u001B[39m\u001B[34m(self, texts, engine, chunk_size, **kwargs)\u001B[39m\n\u001B[32m    477\u001B[39m batched_embeddings: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfloat\u001B[39m]] = []\n\u001B[32m    478\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m _iter:\n\u001B[32m--> \u001B[39m\u001B[32m479\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    480\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m=\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43m_chunk_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mclient_kwargs\u001B[49m\n\u001B[32m    481\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    482\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[32m    483\u001B[39m         response = response.model_dump()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/openai/resources/embeddings.py:132\u001B[39m, in \u001B[36mEmbeddings.create\u001B[39m\u001B[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m    126\u001B[39m             embedding.embedding = np.frombuffer(  \u001B[38;5;66;03m# type: ignore[no-untyped-call]\u001B[39;00m\n\u001B[32m    127\u001B[39m                 base64.b64decode(data), dtype=\u001B[33m\"\u001B[39m\u001B[33mfloat32\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    128\u001B[39m             ).tolist()\n\u001B[32m    130\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[32m--> \u001B[39m\u001B[32m132\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    133\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/embeddings\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    134\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mEmbeddingCreateParams\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    135\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    136\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    137\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    138\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    139\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    140\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpost_parser\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    141\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    142\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mCreateEmbeddingResponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    143\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/openai/_base_client.py:1259\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1245\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1246\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1247\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1254\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1255\u001B[39m ) -> ResponseT | _StreamT:\n\u001B[32m   1256\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1257\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001B[32m   1258\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1259\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/openai/_base_client.py:1052\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1049\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m   1051\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mcould not resolve response (should never happen)\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1052\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_process_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1053\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1054\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1055\u001B[39m \u001B[43m    \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1056\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1057\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1058\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1059\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/openai/_base_client.py:1141\u001B[39m, in \u001B[36mSyncAPIClient._process_response\u001B[39m\u001B[34m(self, cast_to, options, response, stream, stream_cls, retries_taken)\u001B[39m\n\u001B[32m   1138\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(response.request.headers.get(RAW_RESPONSE_HEADER)):\n\u001B[32m   1139\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, api_response)\n\u001B[32m-> \u001B[39m\u001B[32m1141\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mapi_response\u001B[49m\u001B[43m.\u001B[49m\u001B[43mparse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/openai/_response.py:325\u001B[39m, in \u001B[36mAPIResponse.parse\u001B[39m\u001B[34m(self, to)\u001B[39m\n\u001B[32m    323\u001B[39m parsed = \u001B[38;5;28mself\u001B[39m._parse(to=to)\n\u001B[32m    324\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_given(\u001B[38;5;28mself\u001B[39m._options.post_parser):\n\u001B[32m--> \u001B[39m\u001B[32m325\u001B[39m     parsed = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_options\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpost_parser\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparsed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    327\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(parsed, BaseModel):\n\u001B[32m    328\u001B[39m     add_request_id(parsed, \u001B[38;5;28mself\u001B[39m.request_id)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/pyth313/lib/python3.13/site-packages/openai/resources/embeddings.py:115\u001B[39m, in \u001B[36mEmbeddings.create.<locals>.parser\u001B[39m\u001B[34m(obj)\u001B[39m\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_given(encoding_format):\n\u001B[32m    112\u001B[39m     \u001B[38;5;66;03m# don't modify the response object if a user explicitly asked for a format\u001B[39;00m\n\u001B[32m    113\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[32m--> \u001B[39m\u001B[32m115\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m:\n\u001B[32m    116\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mNo embedding data received\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m embedding \u001B[38;5;129;01min\u001B[39;00m obj.data:\n",
      "\u001B[31mAttributeError\u001B[39m: 'str' object has no attribute 'data'"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
