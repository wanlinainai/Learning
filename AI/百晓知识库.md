# 百晓知识库

## 知识管理平台

![image-20251109191638565](images/百晓知识库/image-20251109191638565.png)







## 智能对话

![image-20251109192044176](images/百晓知识库/image-20251109192044176.png)





## 知识库

![image-20251109192436401](images/百晓知识库/image-20251109192436401.png)





## 联网搜索

![image-20251109192909173](images/百晓知识库/image-20251109192909173.png)





## 智能推荐

![image-20251109193141304](images/百晓知识库/image-20251109193141304.png)

## 技术选择性

| 项目     | 作用                                     | 技术栈考虑（考虑数据安全）                                   | 技术栈推荐（不考虑数据安全）                                 |
| -------- | ---------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 知识管理 | 1. 完成数据工程<br />2. 完成知识增删改查 | 1. PDF文件解析：目标检测模型（YOLOX）和OCR模型<br />2. 开发框架：Langchain、LlamaIndex等等<br />3. embedding模型：尽量适配选用的大模型 | 1. PDF文件解析：基于多模态大模型，如GPT-4o<br />2. 开发框架：Langchain、LlamaIndex等等<br />3. Embedding模型：最好与大模型适配 |
| 知识检索 | 1. 知识检索                              | 1. 大模型：开源模型（Qwen系列、GLM系列等）<br />2. 向量数据库：FAISS、Milvus<br />3. 开发框架：Langchain、LlamaIndex<br />4. re-rank模型：bge-reranker-large<br />5. embedding模型：跟知识管理平台一个模型<br />6. 前后端框架：VUE + FastAPI | 1. 大模型：在线调试（GPT4o、GLM）等<br />2. 向量数据库：FAISS、Milvus、TencentCloud VecrorDB<br />3. 开发框架：Langchain、LlamaIndex，如果需求比较简单可以使用Coze、dify平台<br />4. re-ranker模型：bge-reranker-large<br />5. Embedding模型：跟知识管理平台一个模型<br />6. 前后端框架：VUE + FastAPI |
| 效果评估 | 知识效果评估                             | 1. 人工打分<br />2. 基于开源大模型自研评估逻辑               | 1. 人工打分<br />2. 基于在线大模型自研评估逻辑<br />3. RAG评估开源工具，trulens/ragas |

> 最终选型：
>
> | 技术点        | 选型                                 |
> | ------------- | ------------------------------------ |
> | 大模型        | 开源的：GLM4-9B<br />在线模型：GLM-4 |
> | Embedding模型 | Embedding-2                          |
> | 开发框架      | Langchain                            |
> | 向量数据库    | Milvus                               |
> | re-ranker     | 自研算法                             |
> | 效果评估      | 自研框架                             |
> | 前端框架      | VUE                                  |
> | 后端          | FastAPI                              |
> | PDF解析       | 目标检测模型和OCR模型                |

## 开发：核心技术

| 分类         | 描述                 | 地址                                                   | 知识点                                                      |
| ------------ | -------------------- | ------------------------------------------------------ | ----------------------------------------------------------- |
| 数据库       | Python操作           | https://www.sqlalchemy.org/                            | 基于Sqlalchemy持久化存储会话信息                            |
| FastChat     | 模型管理             | https://github.com/lm-sys/FastChat                     | 1. FastChat的原理<br />2. 集成在线模型<br />3. 集成开源模型 |
| 进程与多线程 | Python实现服务并行   | https://docs.python.org/3/library/multiprocessing.html | 学习如何使用multiprocessing做服务并行                       |
| 异步编程     | Python如何写异步代码 | https://docs.python.org/3.11/library/asyncio.html      | 了解Python异步代码风格                                      |



### 数据库构建

- user

```shell
# 主键id、用户名、密码哈希、聊天对话、知识库相关
id 
username
password_hash
conversations
knowledge_bases
```

- conversation（会话模型，表示用户的一个聊天会话）

```shell
# 主键id、用户id、对话框的名称、聊天类型、创建时间、会话和用户多对一的关系、会话和消息一对多的关系
id
user_id
name
chat_type
create_time
user # 会话和用户关系
messages # 会话和消息对应关系
```

- message（聊天记录模型，会话中的一条消息内容）

```shell
# 主键id、会话id、会话关系、聊天类型、用户问题、模型回答、知识库信息、用户评分、用户评分理由、创建时间
id
conversation_id
conversation # 会话关系
chat_type
query
response
meta_data
feedback_score # 用户评分，用于推荐系统
feedback_reason # 用户评分理由，用于推荐系统
create_time
```

> 与知识库相关的库设计：
>
> - knowledge_file（知识文件）
>
> ```shell
> # 主键id、文件名、文件扩展名、所属知识库名称、文档加载器名称、文本切割器名称、文件版本、文件修改时间、文件大小、是否自定义docs、切分文档数量、创建时间
> id 
> file_name
> file_ext
> kb_name
> document_loader_name
> text_splitter_name
> file_version
> file_mtime
> file_size
> custom_docs
> docs_count
> create_time
> ```
>
> - knowledge_base（知识库模型）
>
> ```shell
> # 知识库id、知识库名称、知识库简介（用于Agent）、向量库类型、嵌入模型名称、文件数量、创建时间、用户id（外键）、用户关系
> id
> kb_name
> kb_info
> vs_type
> embed_model
> file_count
> create_time
> user_id
> user
> ```

### FastChat模型管理

FastChat框架部署一个完整的模型服务主要分成了三个部分：Controller、Server、多个Worker。部署关系如下图所示：

![image-20251109223530447](images/百晓知识库/image-20251109223530447.png)

三者的关系是：

- Server部分用于接收请求，并将不同的请求分发到对应的Worker上，对应上图中蓝色的链路Data plane。
- 对于Server，如何知道有哪一些Worker（对应的IP地址），这个时候需要Controller，Controller部分不仅能与Worker通信，同时可以与Server通信：
  - 与Worker部分的通信，是将Worker的信息注册到Controller部分的对应的数据结构中，Controller还需要记录每一个Worker的健康情况；
  - 与Server部分的通信，是让Server查找到对应请求的Worker信息，并将请求转达到具体的Worker。
- Worker是参与真实的模型计算的模块。

**简而言之，Server部分负责接收到用户的请求，并将请求转达到对应的Worker上，在计算完成之后，将Worker返回的结果返回给用户；Worker部分是整个模型计算的核心部分；Controller部分用于保存和更新模型的信息**

**核心概念**

- **Controller**起到了Server和Worker之间的桥梁的作用，一方面是存储模型的相关信息，另一方面是对外提供一些接口。
- **Worker**主要就是加载模型并完成模型的推理任务
- **Server**：在fastChat中提供了两种Server的形式：
  - Web GUI：对应的启动脚本是：`gradio_web_server.py`
  - Web API：对应的启动脚本是：`openai_api_server.py`

![image-20251109224611252](images/百晓知识库/image-20251109224611252.png)

> 随便看看：FastChat部署：
>
> 1. git clone https://github.com/lm-sys/FastChat.git
> 2. conda create -n FastChat --clone hw-chat    # 这是复制一份原本的 conda环境
> 3. conda activate FastChat  
> 4. pip install pydantic_settings
> 5. 启动： python -m fastchat.serve.cli --model-path /root/autodl-tmp/data/ZhipuAI/chatglm3-6b --num-gpus 1
>
> 启动方式二：
>
> 1. 启动Controller：
>
> ```python
> python -m fastchat.serve.controller --host 127.0.0.1
> ```
>
> 2. 启动Model_worker，注册到Controller的服务地址
>
> ```python
> python -m fastchat.serve.model_worker --model-path /root/autodl-tmp/data/ZhipuAI/chatglm3-6b --num-gpus 1 --host 127.0.0.1
> ```
>
> 3. OpenAI API Server 服务，需要绑定到COntroller服务
>
> ```python
> python -m fastchat.serve.openai_api_server --host 127.0.0.1
> ```
>
> 4. 将OpenAI的API Server服务作为接收请求的入口进行调用
>
> ```python
> curl -X POST http://localhost:8000/v1/chat/completions \
> -H "Content-Type: application/json" \
> -d '{
> 	"model": "chatglm3-6b",
>   "messages": [{"role": "user", "content": "请介绍一下你自己？"}]
> }'
> ```
>
> 











































